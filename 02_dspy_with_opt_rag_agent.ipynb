{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8cccd5-b15f-470e-89b3-7c2a6555cfa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimizing with DSPy\n",
    "\n",
    "Most folks who have heard about DSPy are familiar with the fact that DSPy offers automated optimizations - both for prompts and for finetuning. It is definitely one of the standout features of DSPy, as using **[Optimizers](https://dspy.ai/learn/optimization/optimizers/?h=optimizer)** is an extremely powerful way to further improve the quality of your solution. \n",
    "\n",
    "#####Â Note:\n",
    "As we saw in the previous notebook, DSPy offers many other benefits beyond optimization. We emphasize this point because you shouldn't discount using DSPy if your customer doesn't have the training examples needed (we typically recommend 70+ examples) for optimization when you start building your solution. You can still benefit from all the other features of DSPy. \n",
    "\n",
    "**The defining value proposition of DSPy is providing a declarative approach to developing GenAI solutions.**\n",
    "\n",
    "Additionally, while most customers won't have a curated dataset when they start a project, they will want to log interactions for their production applications, which can then be used to create a training set for the DSPy Optimizers.\n",
    "\n",
    "### Optimizing DSPy programs in Databricks\n",
    "\n",
    "With that said, lets look at how you can leverage DSPy's Optimizers.\n",
    "\n",
    "This notebook looks at optimizing the RAG app that we previously built, once again focusing on highlighting the seamless integration with Databricks & Mlflow native features, rather than results (as this uses a dummy dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1465fdd9-25bd-483c-950e-d87b73267d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO change to pypi version when PR merged\n",
    "%pip install -q dspy databricks-agents mlflow databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559bd563-9597-4ef3-a6a4-655fcbcd061e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import math\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "from dspy.retrieve.databricks_rm import DatabricksRM\n",
    "\n",
    "from databricks.agents.evals import judges\n",
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bda06da-ab56-420d-86aa-9db6abf4cc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_file = \"config.yaml\"\n",
    "model_config = ModelConfig(development_config=config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf114abb-cff1-4e80-a4cf-cd1c58aefe0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = model_config.get(\"catalog\")\n",
    "SCHEMA = model_config.get(\"schema\")\n",
    "VOLUME = model_config.get(\"volume\")\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT = model_config.get(\"vector_search_endpoint\")\n",
    "VECTOR_SEARCH_INDEX = model_config.get(\"vector_search_index\")\n",
    "index_path = f\"{CATALOG}.{SCHEMA}.{VECTOR_SEARCH_INDEX}\"\n",
    "\n",
    "model = model_config.get(\"chat_endpoint_name\")\n",
    "\n",
    "LM = f\"databricks/{model}\"\n",
    "\n",
    "path = f\"{CATALOG}/{SCHEMA}/{VOLUME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1017349e-8f3d-497d-8895-edacd9a028a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll start with the same RAG program module we wrote in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9520b2dd-9a03-45a4-adbd-df01b19126fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "  def __init__(self, num_docs=40, for_mosaic_agent=False): \n",
    "    # setup mlflow tracing\n",
    "    mlflow.dspy.autolog()\n",
    "\n",
    "    # setup flag indicating if the object will be deployed as a Mosaic Agent\n",
    "    self.for_mosaic_agent = for_mosaic_agent\n",
    "    # TODO: check with Luis, caching was on by default and didn't require me to set up a volume\n",
    "    self.lm = dspy.LM(LM, cache=False)\n",
    "\n",
    "    # setup the primary retriever pointing to the chunked documents\n",
    "    self.retriever = DatabricksRM(\n",
    "        databricks_index_name=index_path,\n",
    "        text_column_name=\"page_content\",\n",
    "        docs_id_column_name=\"unique_chunk_index\",\n",
    "        # docs_uri_column_name=\"unique_chunk_index\",\n",
    "        columns=[\"source\"],\n",
    "        k=5,\n",
    "        use_with_databricks_agent_framework=for_mosaic_agent # TODO add details from slack thread for use_with_databricks_agent_framework\n",
    "    )\n",
    "    # signature = QnASignature()\n",
    "    self.response_generator = dspy.Predict(\"context, question -> answer, relevant_sources\")\n",
    "\n",
    "\n",
    "  def forward(self, question):\n",
    "    # TODO: review with Luis & Rafi - do we need to manipulate the objects this much or is there a better way?\n",
    "    if self.for_mosaic_agent:\n",
    "      # When using a mosaic agent, questions can be of type ChatAgentMessage or (TODO ChatModelMessage) \n",
    "      question = question[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    context = self.retriever(\n",
    "        question, \n",
    "        query_type=\"hybrid\"# Using hybrid search (embeddings + keywords search)\n",
    "    )\n",
    "\n",
    "    with dspy.context(lm=self.lm):\n",
    "      response = self.response_generator(context=context, question=question)\n",
    "\n",
    "      if self.for_mosaic_agent:\n",
    "        return response.response\n",
    "      return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533c6e9a-f4e6-4db7-8645-c9d2ee1de4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b69bb3-c63e-4de0-b433-7fb882445701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimization breakdown\n",
    "\n",
    "A typical DSPy Optimizer requires three things:\n",
    "\n",
    "1. Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program. In our example, this is our RAG module. \n",
    "2. A curated dataset as training inputs - the more the better, but you can start with what's feasible to create/access.\n",
    "3. Your metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4ec5681-fd6d-4ea4-b71c-fb2644995014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Optimization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712ace55-b6f1-42ac-bf06-ed095913516b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(f\"/Volumes/{path}/DSPy Databricks QnA - Sheet1.csv\", multiLine=True, header=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bfc43f-bbb1-4e29-aacb-2421ade79b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a0ffe1-a1af-4a59-adbd-706c9795dfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DSPy Examples\n",
    "Once we've loaded our dataset, we need to map them to DSPy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5fa6d6-1815-4718-a8f5-c3c8af612545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainset, valset = dataset.randomSplit([0.7, 0.3], seed=15)\n",
    "\n",
    "trainset = trainset.select(\"Question\", \"Answer\").rdd.map(\n",
    "    lambda row: dspy.Example({\"question\": row[\"Question\"], \"answer\": row[\"Answer\"]}).with_inputs(\"question\")\n",
    ").collect()\n",
    "\n",
    "valset = valset.select(\"Question\", \"Answer\").rdd.map(\n",
    "    lambda row: dspy.Example({\"question\": row[\"Question\"], \"answer\": row[\"Answer\"]}).with_inputs(\"question\")\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f9291b-e165-4a1f-9819-50ea110b2479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex = trainset[0]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa048aca-0956-4a0c-a3db-79d229aa57c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbba30e-535f-4af6-b9bb-030b6b8c4bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evalute_using_mosaic_agent(example, pred, trace=None):\n",
    "    # use https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/llm-judge-metrics#call-judges-using-the-python-sdk\n",
    "    # Running evaluation using the Mosaic Agent Evaluation\n",
    "    return judges.correctness(\n",
    "        request=example.question,\n",
    "        response=pred.answer,\n",
    "        expected_response=example.answer,\n",
    "        ).value.name == \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2038b7-7382-42d9-b8d8-5c8968335141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Set up a bootstrap optimizer, which optimizes the RAG program.\n",
    "optimizer = MIPROv2(\n",
    "    metric=evalute_using_mosaic_agent, # Use defined evaluation function\n",
    "    prompt_model=dspy.LM(LM)\n",
    ")\n",
    "\n",
    "# Start a new MLflow run to track all evaluation metrics\n",
    "with mlflow.start_run(run_name=\"dspy_rag_optimization\"):\n",
    "    # Optimize the program by identifying the best few-shot examples for the prompt used by the `response_generator` step\n",
    "    optimized_rag = optimizer.compile(rag, \n",
    "                                    trainset=trainset,\n",
    "                                    max_bootstrapped_demos=3,\n",
    "                                    requires_permission_to_run=False\n",
    "                                    )\n",
    "\n",
    "optimized_rag.save(\"optimized_rag.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d42765-3383-4269-8bff-4993136e4bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_rag = RAG(for_mosaic_agent=True)\n",
    "optimized_rag.load(\"optimized_rag.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3175c93a-f29b-4da2-bf00-a26e9802ce20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_rag({'messages': [{'content': 'What is Databricks?', 'role': 'user'}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181c9a09-3554-49cb-a90d-9b0bc00792ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_table = model_config.get(\"eval_table\")\n",
    "eval_table_path = f\"{CATALOG}.{SCHEMA}.{eval_table}\"\n",
    "eval_df = spark.table(eval_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0230a07-36fd-49fd-a59d-66634a2f9997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e47aa07-47da-48e6-b702-26384770c285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.evaluate(\n",
    "  model=optimized_rag,\n",
    "  data=eval_df,\n",
    "  model_type=\"databricks-agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f903b52e-a9c8-4922-8964-32c7f3eb69f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So far, we built a very simple chain-of-thought module for question answering and evaluated it on a small dataset, but can we do better?\n",
    "\n",
    "In the rest of this guide, we will build a retrieval-augmented generation (RAG) program in DSPy for the same task. We'll see how this can boost the score substantially, then we'll use one of the DSPy Optimizers to compile our RAG program to higher-quality prompts, raising our scores even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9fe3c2-77fe-4c95-9aa5-5c0755dbcb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set up your system's retriever.\n",
    "As far as DSPy is concerned, you can plug in any Python code for calling tools or retrievers. Here, we'll use the Datbricks Vector Search index we set up earlier to execute a Hybrid Semantic Search (using embeddings and keywords)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_dspy_with_opt_rag_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
