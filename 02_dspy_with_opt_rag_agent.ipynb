{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8cccd5-b15f-470e-89b3-7c2a6555cfa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimizing with DSPy\n",
    "\n",
    "Most folks who have heard about DSPy are familiar with the fact that DSPy offers automated optimizations - both for prompts and for finetuning. It is definitely one of the standout features of DSPy, as using **[Optimizers](https://dspy.ai/learn/optimization/optimizers/?h=optimizer)** is an extremely powerful way to further improve the quality of your solution. \n",
    "\n",
    "####Â Note\n",
    "As we saw in the previous notebook, DSPy offers many other benefits beyond optimization. We emphasize this point because you shouldn't discount using DSPy if you don't have the training examples needed (we typically recommend 70+ examples) for optimization when you start building your solution. You can still benefit from all the other features of DSPy, and can skip ahead to notebook 03_register_and_deploy. \n",
    "\n",
    "**The defining value proposition of DSPy is providing a declarative approach to developing GenAI solutions.**\n",
    "\n",
    "Additionally, while most customers won't have a curated dataset when they start a project, they will want to log interactions for their production applications, which can then be used to create a training set for the DSPy Optimizers.\n",
    "\n",
    "### Optimizing DSPy programs in Databricks\n",
    "\n",
    "With that said, lets look at how you can leverage DSPy's Optimizers in Databricks.\n",
    "\n",
    "This notebook looks at optimizing the RAG app that we previously built, once again focusing on highlighting the seamless integration with Databricks & Mlflow native features, rather than the results themselves (as this uses an artificially created training set which hasn't been curated well enough to show material improvement due to Optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1465fdd9-25bd-483c-950e-d87b73267d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q dspy databricks-agents mlflow databricks-sdk==0.50.0 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559bd563-9597-4ef3-a6a4-655fcbcd061e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import math\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "from dspy.retrieve.databricks_rm import DatabricksRM\n",
    "\n",
    "from databricks.agents.evals import judges\n",
    "from databricks.agents.evals import generate_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bda06da-ab56-420d-86aa-9db6abf4cc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_file = \"config.yaml\"\n",
    "model_config = ModelConfig(development_config=config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf114abb-cff1-4e80-a4cf-cd1c58aefe0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = model_config.get(\"catalog\")\n",
    "SCHEMA = model_config.get(\"schema\")\n",
    "VOLUME = model_config.get(\"volume\")\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT = model_config.get(\"vector_search_endpoint\")\n",
    "VECTOR_SEARCH_INDEX = model_config.get(\"vector_search_index\")\n",
    "index_path = f\"{CATALOG}.{SCHEMA}.{VECTOR_SEARCH_INDEX}\"\n",
    "\n",
    "model = model_config.get(\"chat_endpoint_name\")\n",
    "\n",
    "LM = f\"databricks/{model}\"\n",
    "\n",
    "path = f\"{CATALOG}/{SCHEMA}/{VOLUME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b69bb3-c63e-4de0-b433-7fb882445701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimization breakdown\n",
    "\n",
    "A typical DSPy Optimizer requires three things:\n",
    "\n",
    "1. Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program. In our example, this is our RAG module defined above. \n",
    "2. A curated dataset as training inputs - the more the better, but you can start with what's feasible to create/access.\n",
    "3. Your metric - this is a function that evaluates the output of your program, and assigns it a score (higher is better), and is what the Optimizer will optimize for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1017349e-8f3d-497d-8895-edacd9a028a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll start with the same RAG program module we wrote in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9520b2dd-9a03-45a4-adbd-df01b19126fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "  def __init__(self, for_mosaic_agent=True): \n",
    "    # setup mlflow tracing\n",
    "    mlflow.dspy.autolog()\n",
    "\n",
    "    # setup flag indicating if the object will be deployed as a Mosaic Agent\n",
    "    self.for_mosaic_agent = for_mosaic_agent\n",
    "    self.lm = dspy.LM(LM, cache=False)\n",
    "\n",
    "    # setup the primary retriever pointing to the chunked documents\n",
    "    self.retriever = DatabricksRM(\n",
    "        databricks_index_name=index_path,\n",
    "        text_column_name=\"page_content\",\n",
    "        docs_id_column_name=\"unique_chunk_index\",\n",
    "        columns=[\"page_content\"],\n",
    "        k=5,\n",
    "        use_with_databricks_agent_framework=for_mosaic_agent\n",
    "    )\n",
    "    \n",
    "    self.response_generator = dspy.Predict(\"context, question -> response\")\n",
    "\n",
    "  def forward(self, question):\n",
    "    if self.for_mosaic_agent:\n",
    "      # When using a mosaic agent, questions can be of type ChatAgentMessage or (TODO: accept ChatModelMessage) \n",
    "      question = question[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    context = self.retriever(\n",
    "        question\n",
    "    )\n",
    "\n",
    "    with dspy.context(lm=self.lm):\n",
    "      response = self.response_generator(context=context, question=question)\n",
    "\n",
    "      if self.for_mosaic_agent:\n",
    "        return response.response\n",
    "      return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533c6e9a-f4e6-4db7-8645-c9d2ee1de4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag = RAG(for_mosaic_agent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4ec5681-fd6d-4ea4-b71c-fb2644995014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Curated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712ace55-b6f1-42ac-bf06-ed095913516b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(f\"/Volumes/{path}/DSPy Databricks QnA - Sheet1.csv\", multiLine=True, header=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bfc43f-bbb1-4e29-aacb-2421ade79b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a0ffe1-a1af-4a59-adbd-706c9795dfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DSPy Examples\n",
    "Once we've loaded our dataset, we need to convert each row to a DSPy **[Example](https://dspy.ai/learn/evaluation/data/)** in order to pass them to the Optimizer. The easiest way to create an Example is to pass in a dictionary. Examples are just dictionaries with some additional useful utilities. \n",
    "\n",
    "Use `with_inputs(<input>)` to specify which of the provided fields are input fields. Everything else is considered a label or metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5fa6d6-1815-4718-a8f5-c3c8af612545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainset, valset = dataset.randomSplit([0.7, 0.3], seed=15)\n",
    "\n",
    "trainset = trainset.select(\"Question\", \"Answer\").rdd.map(\n",
    "    lambda row: dspy.Example({\"question\": row[\"Question\"], \"response\": row[\"Answer\"]}).with_inputs(\"question\")\n",
    ").collect()\n",
    "\n",
    "valset = valset.select(\"Question\", \"Answer\").rdd.map(\n",
    "    lambda row: dspy.Example({\"question\": row[\"Question\"], \"response\": row[\"Answer\"]}).with_inputs(\"question\")\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc34120-8559-4a43-a118-9857e191620b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets inspect one of our examples to ensure it's formatted right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f9291b-e165-4a1f-9819-50ea110b2479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex = trainset[0]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa048aca-0956-4a0c-a3db-79d229aa57c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7e8c66-e5e8-4193-bc2e-ba23a417dc1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ex.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd924c89-6bc2-43f2-a9d0-26ba8460cff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Metric\n",
    "\n",
    "Now that we have our program, and our curated dataset as DSPy Examples, the last step is to define a **[Metric](https://dspy.ai/learn/evaluation/metrics/)**. \n",
    "\n",
    "From a programming perspective, Metrics are simple. They take Examples and the output from your program, and return a score which quantifies how good the output is against the reference Example. The challenging part is choosing your Metric. For example, accuracy isn't the best metric for long form answers, as you're unlikely to ever get a word for word matching output, even if the response is 'correct'. \n",
    "\n",
    "#### Mosaic Agent Eval Judges\n",
    "\n",
    "You can define your own Metric, as long as it takes an `Example` and a prediction from your program, and outputs a score. In this notebook we'll use the Mosaic Agent Eval LLM correctness judge to define our Metric.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbba30e-535f-4af6-b9bb-030b6b8c4bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evalute_using_mosaic_agent(example, pred, trace=None):\n",
    "    # Ref: https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/llm-judge-metrics#call-judges-using-the-python-sdk\n",
    "    # Running evaluation using the Mosaic Agent Evaluation\n",
    "    return judges.correctness(\n",
    "        request=example.question,\n",
    "        response=pred.response,\n",
    "        expected_response=example.response,\n",
    "        ).value.name == \"YES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f716f0e-820f-448f-a0f6-a891fb999526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Running Optimizer \n",
    "\n",
    "A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify. DSPy offers a number of different Optimizers (with new ones coming out frequently!), but they all have the same interface. In this example we'll use MIPRO with our `evalute_using_mosaic_agent` Metric.\n",
    "\n",
    "We'll save our optimized program as \"optimized_rag.json\" so that we can load and use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2038b7-7382-42d9-b8d8-5c8968335141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Set up a bootstrap optimizer, which optimizes the RAG program.\n",
    "optimizer = MIPROv2(\n",
    "    metric=evalute_using_mosaic_agent, # Use defined evaluation function\n",
    "    prompt_model=dspy.LM(LM)\n",
    ")\n",
    "\n",
    "# Start a new MLflow run to track all evaluation metrics\n",
    "with mlflow.start_run(run_name=\"dspy_rag_optimization\"):\n",
    "    # Optimize the program by identifying the best few-shot examples for the prompt used by the `response_generator` step\n",
    "    optimized_rag = optimizer.compile(rag, \n",
    "                                    trainset=trainset,\n",
    "                                    max_bootstrapped_demos=3,\n",
    "                                    requires_permission_to_run=False\n",
    "                                    )\n",
    "\n",
    "optimized_rag.save(\"optimized_rag.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44596776-691a-43eb-b1e0-7aeeb25c5cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Optimized Program\n",
    "\n",
    "You'll see a new file \"optimized_rag.json\" has been created by running the cell above. You can view the file to see the Optimized prompt that has been created. \n",
    "\n",
    "Lets now load the optimized prompt in order to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d42765-3383-4269-8bff-4993136e4bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_rag = RAG(for_mosaic_agent=True)\n",
    "optimized_rag.load(\"optimized_rag.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3175c93a-f29b-4da2-bf00-a26e9802ce20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = optimized_rag({'messages': [{'content': 'What features are included in the \"Machine learning tutorial\" notebook for the scikit-learn package?', 'role': 'user'}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4348e09-7aa0-4563-a859-4adab3191778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Under the hood \n",
    "As we saw in the previous notebook, the MLFlow trace helps us understand the steps DSPy took under the hood. Since it's the same program, the internal steps are the same.\n",
    "\n",
    "If you click into `LM.__call__` you'll see that the few shot examples created by MIPROv2 are being passed to the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4b7743-a83e-4a10-a20a-428813d0c81f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Just as we evaluated the unoptimized DSPy program against our synthetic evaluation dataset, we can do the same for the optimized program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a83d616-197f-4741-8049-7b6093a10416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load synthetic eval table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181c9a09-3554-49cb-a90d-9b0bc00792ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_table = model_config.get(\"eval_table\")\n",
    "eval_table_path = f\"{CATALOG}.{SCHEMA}.{eval_table}\"\n",
    "eval_df = spark.table(eval_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0230a07-36fd-49fd-a59d-66634a2f9997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faf07a86-80db-4a50-bc47-7e851fe99f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run evaluation\n",
    "\n",
    "We simply pass in the `optimized_rag` module we created and our evaluation dataset to the mlflow `evaluate` call, and we can see the results in the mlflow run under \"traces\". \n",
    "\n",
    "Please note that this is an engineered example and the focus of this notebook is not on the quality of the results (the vector search is quite poor for now!) but to show the dev flow with DSPy and the integration with Databricks native capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e47aa07-47da-48e6-b702-26384770c285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.evaluate(\n",
    "  model=optimized_rag,\n",
    "  data=eval_df,\n",
    "  model_type=\"databricks-agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23144f03-f46b-4c51-88a9-3819d80bfbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen how:\n",
    "1. We can optimize a DSPy program using the Mosaic Agent Eval LLM judges as a metric\n",
    "2. We can save an optimized DSPy program\n",
    "3. We can evaluate the optimized DSPy program against an evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea48ef20-aef8-40a5-bd2d-ec3a4158f3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_dspy_with_opt_rag_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
