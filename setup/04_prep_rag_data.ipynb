{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219112bf-0806-4df3-a58b-110fb389e80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion \n",
    "In this example, for the sake of simplicity we're using extracts of the Databricks docs across the 3 clouds which we have available to us pre-chunked as a jsonl file, which we will ingest the into a delta table. We also have a a few example questions and answers about Databricks features which we'll use to further optimize our prompts. \n",
    "\n",
    "Your data might be in the form of pdfs, jsons, csvs, in which case you will need to take additional steps to extract the text and chunk it to make it available for use in a RAG agent. For more details about chunking and preparing data, see [here](https://docs.databricks.com/aws/en/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ee1183-bc7d-4dfc-84f0-8dfb85b04446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow>=2.18.0 tokenizers torch transformers openpyxl databricks-sdk langchain==0.1.13\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24054b5f-cb35-4d40-bb59-64447d04ef28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4700404-e4b3-4464-9fa2-d90c005354df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_file = \"../config.yaml\"\n",
    "model_config = ModelConfig(development_config=config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc5a82e-9c5f-4f76-8ed1-a385e8784897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = model_config.get(\"catalog\")\n",
    "SCHEMA = model_config.get(\"schema\")\n",
    "VOLUME = model_config.get(\"volume\")\n",
    "path = f\"{CATALOG}/{SCHEMA}/{VOLUME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af001b37-06bc-4646-97da-36a28d07e493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingest the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1be52b-ccec-4c85-85e2-70cd0ccab2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "azure_docs = spark.read.json(f\"/Volumes/{path}/databricks_docs_azure_chunked.jsonl\").drop(\"metadata\")\n",
    "aws_docs = spark.read.json(f\"/Volumes/{path}/databricks_docs_aws_chunked.jsonl\").drop(\"metadata\")\n",
    "gcp_docs = spark.read.json(f\"/Volumes/{path}/databricks_docs_gcp_chunked.jsonl\").drop(\"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8090a18f-acb8-4c63-8ad7-d5c2f6ab7598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure schemas are aligned\n",
    "aws_docs = aws_docs.select(sorted(aws_docs.columns))\n",
    "gcp_docs = gcp_docs.select(sorted(gcp_docs.columns))\n",
    "azure_docs = azure_docs.select(sorted(azure_docs.columns))\n",
    "\n",
    "# Merge DataFrames using unionByName\n",
    "all_docs = aws_docs.unionByName(gcp_docs).unionByName(azure_docs)\n",
    "\n",
    "display(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d6ae24-35c8-436b-8d30-870c3059df9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC locations to store the chunked documents\n",
    "DBDOCS_CHUNKS_DELTA_TABLE = f\"{CATALOG}.{SCHEMA}.db_docs_bronze\"\n",
    "print(DBDOCS_CHUNKS_DELTA_TABLE)\n",
    "\n",
    "# Ensure unique column names after transformations\n",
    "all_docs = all_docs.withColumnRenamed(\"chunk_index\", \"unique_chunk_index\")\n",
    "\n",
    "all_docs.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DBDOCS_CHUNKS_DELTA_TABLE)\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {DBDOCS_CHUNKS_DELTA_TABLE} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_prep_rag_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
